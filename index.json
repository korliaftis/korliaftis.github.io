[{"content":"Sometimes everything looks fine on the surface but appearances can be deceiving. Here we will see how to set up the Prometheus monitoring stack to monitor our stuff and send alerts when needed.\nThe core component of the Prometheus monitoring stack is the Prometheus server. We configure targets for the server (meaning endpoints that expose metrics) and the server connects to them at regular intervals to scrape their metrics and store them locally in a time series database (TSDB).\nWe also configure rules for the server. Rules are PromQL expressions that run at regular intervals against the data stored in the TSDB and trigger alerts when certain conditions are met.\nWhen an alert is triggered the server forwards it to the Alertmanager which is another component of the Prometheus monitoring stack. The Alertmanager applies some routing and grouping logic before sending a notification out to its configured receivers.\nSome systems and services do not expose metrics by default. In these cases we use Prometheus exporters which are installed on systems or alongside services to collect metrics and expose them through endpoints. These endpoints can then be configured as targets for the Prometheus server.\nLet\u0026rsquo;s see some more details about each component we will use..\nPrometheus Server The script we will use to install the Prometheus server will place all the configuration files under the /etc/prometheus/ directory..\n. |-- prometheus.yaml # configuration file |-- rules # rule files | |-- alertmanager.yaml | |-- blackbox_exporter.yaml | |-- node_exporter.yaml | |-- prometheus.yaml | `-- watchdog.yaml `-- targets # target files |-- servers | `-- server.yaml |-- services | |-- alertmanager.yaml | |-- blackbox_exporter.yaml | |-- cadvisor.yaml | |-- docker.yaml | |-- haproxy.yaml | `-- prometheus.yaml `-- sites `-- google.yaml The main configuration file for the server is the /etc/prometheus/prometheus.yaml where we configure things like..\nhow often to scrape targets how often to evaluate rules where is the Alertmanager where are the rules where are the targets Note that any change we make to the /etc/prometheus/prometheus.yaml file will not take effect until we restart the Prometheus systemd service..\nsystemctl restart prometheus.service On the /etc/prometheus/prometheus.yaml file we have configured three scrape jobs with the names \u0026ldquo;servers\u0026rdquo;, \u0026ldquo;services\u0026rdquo; and \u0026ldquo;sites\u0026rdquo;. These jobs are using the file based discovery mechanism which allows us to add, update and remove targets without having to restart the Prometheus systemd service each time.\nThe target files are stored under /etc/prometheus/targets/ and we categorize them with the following convention..\n/etc/prometheus/targets/servers/ node exporters for physical and virtual machines /etc/prometheus/targets/services/ software that exposes metrics /etc/prometheus/targets/sites/ targets for the blackbox exporter The script will create a few target files for us. These target files assume that everything is installed on the same machine so they are all pointing to localhost. If this is not the case or if you don\u0026rsquo;t want to scrape metrics from a specific target simply edit or remove them.\nTo add new targets we create files under the relevant directory..\nto add a server..\nTARGET_IP=\u0026#34;\u0026#34; # 10.0.0.3 TARGET_PORT=\u0026#34;\u0026#34; # 9100 TARGET_NAME=\u0026#34;\u0026#34; # server cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/prometheus/targets/servers/${TARGET_NAME}.yaml - targets: - \u0026#34;${TARGET_IP}:${TARGET_PORT}\u0026#34; labels: instance: \u0026#34;${TARGET_NAME}\u0026#34; EOF to add a service..\nTARGET_IP=\u0026#34;\u0026#34; # 127.0.0.1 TARGET_PORT=\u0026#34;\u0026#34; # 9323 TARGET_NAME=\u0026#34;\u0026#34; # docker cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/prometheus/targets/servers/${TARGET_NAME}.yaml - targets: - \u0026#34;${TARGET_IP}:${TARGET_PORT}\u0026#34; labels: instance: \u0026#34;${TARGET_NAME}\u0026#34; EOF to add a site..\nTARGET_URL=\u0026#34;\u0026#34; # https://google.com TARGET_NAME=\u0026#34;\u0026#34; # google cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/prometheus/targets/sites/${TARGET_NAME}.yaml - targets: - \u0026#34;${TARGET_URL}\u0026#34; EOF The filenames don\u0026rsquo;t matter and we can have files with multiple targets under them. The IP address we provide for TARGET_IP needs to be reachable by the Prometheus server. We give a value for the instance label so it doesn\u0026rsquo;t automatically get the value from the __address__ label (more info on that here).\nIf we want to add a new target that is not compatible with the existing scrape jobs we can add it as a new scrape job in the prometheus.yaml file.\nThe script will also create rule files for us under the /etc/prometheus/rules/ directory. We can find a collection of alert rules here. Note that any change we make to the rule files will not take effect until we restart the Prometheus systemd service.\nAlertmanager The script we will use to install the Alertmanager will place all configuration files under the /etc/alertmanager/ directory. On the /etc/alertmanager/alertmanager.yaml file we configure things like..\nwhere are the notification templates how to route alerts to receivers which are the receivers for the notifications Note that any change we make to the /etc/alertmanager/alertmanager.yaml file will not take effect until we restart the Alertmanager systemd service..\nsystemctl restart alertmanager.service The script will create a minimal configuration that works well for \u0026ldquo;slow\u0026rdquo; environments. We will use a single notification template and route all alerts (except the Watchdog alert) to a Discord webhook.\nThe Watchdog alert is a dead man\u0026rsquo;s switch type of alert designed to always fire. When it stops firing it means that something is wrong with the monitoring system. We will route this alert to a healthchecks.io webhook that will notify us only when it stops firing.\nNode Exporter This exporter exposes system metrics from a host like CPU, memory, disk and network usage. Using these metrics we can understand the overall health of the system and how its resources are being used.\nBlackbox Exporter This exporter can be used to expose metrics for a wide range of targets but we mainly use it to monitor HTTP endpoints and collect information about availability, latency and TLS certificate expiration.\nCompared to other exporters the Blackbox Exporter works differently. It comes bundled with modules that define different types of checks. When the Prometheus server scrapes the Blackbox Exporter it provides the target and the module to use for the specific check.\ncAdvisor This exporter exposes metrics for all containers running on a system. We could already expose metrics for the Docker engine by adding the metrics-addr option in the /etc/docker/daemon.json configuration file but with cAdvisor we can expose metrics for the actual containers.\nInstallation The scripts below can be executed on both x86 and ARM systems so you can also run them on devices like the Raspberry Pi.\nThe binaries are placed in /usr/local/bin/, systemd files in /etc/systemd/system/, configuration files in /etc/ and storage files in /var/lib/.\nThe systemd files use the 0.0.0.0 address, if you want to limit access to a specific interface either update them or use a firewall.\nAll components create their own service accounts except cAdvisor that uses the root account.\nprometheus server USERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/prometheus.sh https://blog.korliaftis.com/scripts/prometheus.sh chmod +x /home/${USERNAME}/.scripts/prometheus.sh sudo /home/${USERNAME}/.scripts/prometheus.sh The metrics and dashboard are accessible on port 9090.\nalertmanager USERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/alertmanager.sh https://blog.korliaftis.com/scripts/alertmanager.sh chmod +x /home/${USERNAME}/.scripts/alertmanager.sh sudo /home/${USERNAME}/.scripts/alertmanager.sh The metrics and dashboard are accessible on port 9093. After the installation, update /etc/alertmanager/alertmanager.yaml with your webhooks if you want to receive notifications.\nnode exporter USERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/node-exporter.sh https://blog.korliaftis.com/scripts/node-exporter.sh chmod +x /home/${USERNAME}/.scripts/node-exporter.sh sudo /home/${USERNAME}/.scripts/node-exporter.sh The metrics and dashboard are accessible on port 9100.\nblackbox exporter USERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/blackbox-exporter.sh https://blog.korliaftis.com/scripts/blackbox-exporter.sh chmod +x /home/${USERNAME}/.scripts/blackbox-exporter.sh sudo /home/${USERNAME}/.scripts/blackbox-exporter.sh The metrics and dashboard are accessible on port 9115.\ncadvisor USERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/cadvisor.sh https://blog.korliaftis.com/scripts/cadvisor.sh chmod +x /home/${USERNAME}/.scripts/cadvisor.sh sudo /home/${USERNAME}/.scripts/cadvisor.sh The metrics and dashboard are accessible on port 9324.\nTroubleshooting If you create a target and it does not appear on the Prometheus dashboard at the URL http://${PROMETHEUS_SERVER_IP}:9090/targets it means that there is an issue with the file you created in /etc/prometheus/targets.\nIf you create a target and it appears on the Prometheus dashboard but its state is DOWN it means that either Prometheus can\u0026rsquo;t reach the ${TARGET_IP} or that the metrics are not exposed at the URL http://${TARGET_IP}:${TARGET_PORT}/metrics.\nIf you create a rule file and it doesn\u0026rsquo;t appear on the Prometheus dashboard at the URL http://${PROMETHEUS_SERVER_IP}:9090/rules restart the Prometheus systemd service.\nIf you see alerts firing at the URL http://${PROMETHEUS_SERVER_IP}:9090/alerts but you are not receiving notifications on Discord, check that..\nthere is no issue with the configuration file of the Alertmanager.. amtool check-config /etc/alertmanager/alertmanager.yaml the Alertmanager service is running.. systemctl status alertmanager in /etc/prometheus/prometheus.yaml under the alerting: section the IP of the Alertmanager is correct and reachable by the system hosting the Prometheus server in /etc/alertmanager/alertmanager.yaml you have configured your Discord webhook If cAdvisor is using too much CPU check this GitHub issue.\nOngoing Maintenance The scripts we used for the installation can also upgrade the components they install. On the first run they perform a full installation, subsequent runs upgrade the components to their latest versions without changing their configuration files.\n","permalink":"https://korliaftis.github.io/posts/monitoring-with-prometheus.html","summary":"\u003cp\u003eSometimes everything looks fine on the surface but appearances can be deceiving. Here we will see how to set up the Prometheus monitoring stack to monitor our stuff and send alerts when needed.\u003c/p\u003e\n\u003cp\u003eThe core component of the Prometheus monitoring stack is the Prometheus server. We configure targets for the server (meaning endpoints that expose metrics) and the server connects to them at regular intervals to scrape their metrics and store them locally in a time series database (TSDB).\u003c/p\u003e","title":"Monitoring with Prometheus"},{"content":"☝️\n","permalink":"https://korliaftis.github.io/posts/diving-into-rust.html","summary":"Everything we have done so far was just setting the stage for our real goal, which is getting our hands dirty with the Rust programming language. It\u0026rsquo;s a modern engineering marvel destined to make all other languages obsolete. Either we jump on the Rust wagon now or get left behind with the memory-unsafe losers.","title":"Diving into Rust"},{"content":"We now have a working system with several tools installed but in most cases we want these tools to work together and reach a specific state before we can start using them. Once we define what that state should look like we also need a simple way to create and tear down environments as needed.\nFor my use case, that state includes a Kubernetes cluster running Traefik, Prometheus and Argo CD. I automate the environment creation process with a Bash script and Terraform.\nTo run the script, you need:\na domain with A records pointing to your system’s local IP address a wildcard TLS certificate a GitHub account with at least one repository your SSH key added to the GitHub account To download the script run the following..\nUSERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/minikube.sh https://blog.korliaftis.com/scripts/minikube.sh chmod +x /home/${USERNAME}/.scripts/minikube.sh Before running the script, make sure to edit it and update the following variables..\nDOMAIN_NAME=\u0026#34;\u0026#34; # your domain name - \u0026#34;domain.xyz\u0026#34; GITHUB_USERNAME=\u0026#34;\u0026#34; # your github username - \u0026#34;user\u0026#34; GITHUB_REPOSITORY=\u0026#34;\u0026#34; # your github repository name - \u0026#34;argocd\u0026#34; MINIKUBE_NODES=\u0026#34;\u0026#34; # the number of minikube nodes - \u0026#34;3\u0026#34; TLS_PUBLIC_KEY_PATH=\u0026#34;\u0026#34; # the full path to the tls public key - \u0026#34;/home/user/.tls/fullchain.cer\u0026#34; TLS_PRIVATE_KEY_PATH=\u0026#34;\u0026#34; # the full path to the tls private key - \u0026#34;/home/user/.tls/domain.xyz.key\u0026#34; SSH_PRIVATE_KEY_PATH=\u0026#34;\u0026#34; # the full path to the ssh private key - \u0026#34;/home/user/.ssh/id_ed25519\u0026#34; In summary, the script first performs a few preflight checks to ensure everything is in order, it then generates Terraform manifests in the ${HOME}/minikube directory and finally applies them in two stages to avoid dependency errors.\nWhen you run the script it will delete all existing Minikube clusters and contents from the ${HOME}/minikube directory.\nIn the following sections we will see more details about some key components.\nminikube We use Minikube with the Docker driver. During installation we created a custom Docker network named minikube allowing the nodes to receive predictable IP addresses.\nThe number of nodes to be created is defined by the MINIKUBE_NODES variable in the script.\nSome of the workloads we deploy require at least 2 nodes otherwise some pods will not be scheduled and remain in a Pending state.\nhaproxy To access HTTP workloads running on Minikube we use the HAProxy we installed on our system.\nIt listens on ports 80 and 443 of the host system and forwards all incoming traffic to the minikube Docker network on ports 31080 and 31443 respectively. For this setup to work, the Kubernetes Ingress Controller (Istio, Traefik, etc) must be configured to create NodePort services on those ports.\nWe have configured HAProxy with active TCP healthchecks so it routes traffic only to backends that have a server running. I have tried some alternatives to HAProxy but none of them could do this simple thing out of the box.. for example NGINX only offers passive TCP healthchecks in it\u0026rsquo;s free version and for active TCP healthchecks we need to use their paid version. I also tried Traefik but as of today it doesn\u0026rsquo;t support any TCP healthchecks for it\u0026rsquo;s backends.\nIn general HAProxy just works, we can access it\u0026rsquo;s dashboard in port 8404 and view it\u0026rsquo;s logs by doing..\nsudo journalctl -u haproxy -f One alternative approach to the whole \u0026ldquo;reverse proxy in front of Minikube\u0026rdquo; thing would be to use MetalLB but I could not make it work consistently with Minikube. That said, it\u0026rsquo;s been some time since I last tested it and it may work great now.\nOne disadvantage of this setup is that ports 80 and 443 of the host system are always bound to HAProxy even when Minikube is not running. If this is something that bothers you, disable the autostart of the HAProxy systemd service and start it only when you need it or even better use a different network interface for HAproxy.\ntraefik As discussed in the HAProxy section, our Ingress Controller needs to use NodePort services on ports 31080 and 3144 so we configure this using the Traefik Helm chart values file.\nAnother thing we configure is a Kubernetes secret on the traefik namespace with the certificate for our domain. Traefik will use this secret to create a TLSStore which will handle TLS for our domain automatically.\nAfter the installation an IngressClass with the name \u0026ldquo;traefik\u0026rdquo; will be created and it will be configured to be the default IngressClass of the cluster. This means that we don\u0026rsquo;t have to set the ingressClassName: field in every Ingress resource we create.\nIf in the future we decide to use multiple Ingress Controllers we can use this field to select which one will handle each Ingress in a case by case basis..\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: # [...] spec: ingressClassName: \u0026#34;traefik\u0026#34; # [...] One final note, Traefik evaluates routing rules based on their length and not based on their \u0026ldquo;specificity\u0026rdquo; meaning that if we create an Ingress for the host blue.domain.xyz and another one for the host *.domain.xyz the rules that will be created will be..\nHost('blue.domain.xyz' HostRegexp('{subdomain:[a-zA-Z0-9-]+}.domain.xyz') The second rule is longer so it will be evaluated first and since it will match the request to blue.domain.xyz it will always take precedence over the more specific route.\nTo solve this we can manually set the priority on the Ingress using an annotation..\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: # [...] annotations: traefik.ingress.kubernetes.io/router.priority: \u0026#34;1\u0026#34; # [...] spec: # [...] The priority can be any value between 1 and 1000 (higher is better). We could either annotate the \u0026ldquo;wildcard\u0026rdquo; Ingress with a low priority or annotate the \u0026ldquo;non-wildcard\u0026rdquo; Ingress with a high priority, either way our problem will be solved.\nprometheus We install 2 Helm charts related to Prometheus, the prometheus-operator-crds and the kube-prometheus-stack.\nWe mostly use default values for both except from the way the Prometheus operator detects ServiceMonitor resources. The default behavior is for Prometheus to scrape a ServiceMonitor only when it has matching labels. This is useful if we have multiple Prometheus operators on the same cluster.\nIn summary the way it works is the following..\nif serviceMonitorSelectorNilUsesHelmValues: false is configured in the values file we need to run kubectl get prometheuses.monitoring.coreos.com -n monitoring -o yaml | grep -A8 serviceMonitorSelector and add the output as labels to our ServiceMonitor if serviceMonitorSelectorNilUsesHelmValues: true is configured in the values file the ServiceMonitor will be scraped automatically In either case the service monitors can be in any namespace.\nAfter the installation we can access Prometheus at https://prometheus.${DOMAIN_NAME} and Alertmanager at https://alertmanager.${DOMAIN_NAME}.\nargo cd We install the argo-cd Helm chart and we just configure ingress related settings through the values file. Additionally we create a Kubernetes secret to store the GitHub account credentials and repository information.\nThe web interface is available at https://argocd.${DOMAIN_NAME}, to login use admin as the username and the output of the following command as the password..\nkubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 --decode \u0026amp;\u0026amp; echo troubleshooting Usually everything works fine but if there is any issue, start troubleshooting the path of the requests..\nIs DNS pointing to the IP of our system? dig ${DOMAIN_NAME} dig *.${DOMAIN_NAME} Does HAProxy see the backends as online? go with your browser to the IP of your system in port 8404 - http://192.168.1.2:8404 Do the HAProxy logs show our connection attempts? sudo journalctl -u haproxy -f Have all the Helm charts installed correctly? helm list -Aa Are all the pods running? kubectl get pods -A This is not an exhaustive list of everything that could have gone wrong but most likely it\u0026rsquo;s one of these things.\nongoing maintenance To keep everything happy we need to edit the script and keep the following up to date..\nTerraform providers\nkubernetes helm Helm charts\ntraefik traefik-crds kube-prometheus-stack prometheus-operator-crds argo-cd Finally, if you have not automated the process, don\u0026rsquo;t forget to generate a new TLS certificate for your domain every 90 days.\n","permalink":"https://korliaftis.github.io/posts/connecting-the-dots.html","summary":"\u003cp\u003eWe now have a working system with several tools installed but in most cases we want these tools to work together and reach a specific state before we can start using them. Once we define what that state should look like we also need a simple way to create and tear down environments as needed.\u003c/p\u003e\n\u003cp\u003eFor my use case, that state includes a Kubernetes cluster running Traefik, Prometheus and Argo CD. I automate the environment creation process with a Bash script and Terraform.\u003c/p\u003e","title":"Connecting the Dots"},{"content":"Since I started working with computers I always loved the idea of a homelab, a safe space to mess with stuff, break stuff and sometimes learn a few things along the way.\nBack when I was trying to land my first tech job my homelab was a bunch of noisy Cisco switches and routers. Along the way I discovered GNS3 and eventually those physical switches and routers were virtualized on my Sony Vaio VGN-N31Z with it\u0026rsquo;s Core 2 Duo, 2GB of RAM and 120GB of spinning rust.\nA few years later when I started working with Windows my homelab transitioned into a Windows Server 2012 R2 virtual machine running on VirtualBox. It didn’t take long to realize that if I wanted to run more than just a couple of VMs at the same time I would need some serious hardware. So I ended up breaking the bank on a desktop running the free version of VMware ESXi packed with an Intel i7, 64GB of RAM and a bunch of SSDs.\nFast forward again and with everything moving to the public cloud my homelab shifted to an AWS account. What was once a capital expense with finite resources became an operational expense with \u0026ldquo;limitless\u0026rdquo; capabilities.\nThese days my homelab is a vintage Dell Latitude with an Intel i3, 16GB of RAM and a 1TB SSD running Debian. Here is how I set it up..\nFoundation This guide begins after we have selected the hardware, downloaded a Debian 13 ISO, created a bootable USB and completed a minimal installation with only the \u0026ldquo;standard system utilities\u0026rdquo; and \u0026ldquo;SSH server\u0026rdquo; selected.\nBy now we should know our machine’s IP address on the network and have successfully logged in via SSH.\nLet’s get started..\nswitch to the root user..\nsu - set the hostname..\nHOSTNAME=\u0026#34;server\u0026#34; hostnamectl set-hostname ${HOSTNAME} set the timezone..\nTIMEZONE_REGION=\u0026#34;Europe\u0026#34; TIMEZONE_CITY=\u0026#34;Athens\u0026#34; timedatectl set-timezone ${TIMEZONE_REGION}/${TIMEZONE_CITY} install packages..\napt update \u0026amp;\u0026amp; apt upgrade -y apt install -y bash-completion ca-certificates apt-transport-https vim git curl jq gnupg unzip htop tree tmux nmap tcpdump ethtool strace uuid-runtime yq configure sudo..\nUSERNAME=\u0026#34;user\u0026#34; apt install -y sudo usermod -a -G sudo ${USERNAME} enable sudo without password..\nUSERNAME=\u0026#34;user\u0026#34; echo \u0026#34;${USERNAME} ALL=(ALL) NOPASSWD:ALL\u0026#34; | EDITOR=\u0026#39;tee -a\u0026#39; visudo visudo --check configure ssh..\nUSERNAME=\u0026#34;user\u0026#34; install -d -m 0700 -o ${USERNAME} -g ${USERNAME} /home/${USERNAME}/.ssh install -m 0600 -o ${USERNAME} -g ${USERNAME} /dev/null /home/${USERNAME}/.ssh/config install -m 0600 -o ${USERNAME} -g ${USERNAME} /dev/null /home/${USERNAME}/.ssh/authorized_keys su - ${USERNAME} -c \u0026#34;ssh-keygen -t ed25519 -f /home/${USERNAME}/.ssh/id_ed25519 -N \u0026#39;\u0026#39;\u0026#34; sed -i \u0026#39;/^#/! {/AcceptEnv/ s/^/#/}\u0026#39; /etc/ssh/sshd_config enable ssh without password (if the key we use is also configured on GitHub)..\nUSERNAME=\u0026#34;user\u0026#34; GITHUB_USERNAME=\u0026#34;torvalds\u0026#34; curl https://github.com/${GITHUB_USERNAME}.keys \u0026gt;\u0026gt; /home/${USERNAME}/.ssh/authorized_keys set the global git email address for our user account..\nUSERNAME=\u0026#34;user\u0026#34; GITHUB_EMAIL=\u0026#34;torvalds@users.noreply.github.com\u0026#34; su - ${USERNAME} -c \u0026#34;git config --global user.email ${GITHUB_EMAIL}\u0026#34; change the way the bash prompt looks..\nUSERNAME=\u0026#34;user\u0026#34; echo \u0026#39;export PS1=\u0026#34;\\[\\e[37m\\][\\[\\e[93m\\]\\w\\[\\e[37m\\]] \\[\\e[0m\\]\u0026#34;\u0026#39; \u0026gt;\u0026gt; /home/${USERNAME}/.bashrc echo \u0026#39;export PS1=\u0026#34;\\[\\e[37m\\][\\[\\e[91m\\]\\w\\[\\e[37m\\]] \\[\\e[0m\\]\u0026#34;\u0026#39; \u0026gt;\u0026gt; /root/.bashrc create a few files and directories to keep our stuff organized..\nUSERNAME=\u0026#34;user\u0026#34; install -d -m 0775 -o ${USERNAME} -g ${USERNAME} /home/${USERNAME}/.tls install -d -m 0775 -o ${USERNAME} -g ${USERNAME} /home/${USERNAME}/.scripts install -m 0644 -o ${USERNAME} -g ${USERNAME} /dev/null /home/${USERNAME}/.bash_aliases disable the laptop lid function (if we use a laptop)..\necho \u0026#39;HandleLidSwitch=ignore\u0026#39; \u0026gt;\u0026gt; /etc/systemd/logind.conf suppress the output of kernel logs on the console..\necho \u0026#39;kernel.printk = 1 4 1 7\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p increase the /proc/sys/fs/inotify/max_user_instances value..\necho \u0026#39;fs.inotify.max_user_instances = 16384\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p update the password of our user account..\nUSERNAME=\u0026#34;user\u0026#34; passwd ${USERNAME} update the password of the root account..\npasswd root Building Blocks Now that we have established the foundation, we can begin adding our building blocks.\nSince I work as a Site Reliability Engineer the tools I use include the following..\ndocker USERNAME=\u0026#34;user\u0026#34; curl -L https://download.docker.com/linux/debian/gpg | gpg --dearmor \u0026gt; /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${VERSION_CODENAME}\u0026#34;) stable\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list apt update \u0026amp;\u0026amp; apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin usermod -aG docker ${USERNAME} systemctl daemon-reload systemctl enable docker.service systemctl start docker.service By default Docker binds published ports to all interfaces (0.0.0.0) making containers accessible from anywhere. I prefer the opposite, containers should be private unless explicitly exposed.\nsystemctl stop docker.service cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; /etc/docker/daemon.json { \u0026#34;ip\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;metrics-addr\u0026#34;: \u0026#34;0.0.0.0:9323\u0026#34; } EOF systemctl start docker.service kubectl TEMP_DIR=\u0026#34;$(mktemp -d)\u0026#34; curl -Lo ${TEMP_DIR}/kubectl \u0026#34;https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; install -m 0755 -o root -g root ${TEMP_DIR}/kubectl /usr/local/bin/kubectl kubectl completion bash \u0026gt; /etc/bash_completion.d/kubectl k9s TEMP_DIR=\u0026#34;$(mktemp -d)\u0026#34; K9S_VERSION=\u0026#34;$(curl -Ls https://api.github.com/repos/derailed/k9s/releases/latest | jq -r .tag_name)\u0026#34; curl -Lo ${TEMP_DIR}/k9s_Linux_amd64.tar.gz https://github.com/derailed/k9s/releases/download/${K9S_VERSION}/k9s_Linux_amd64.tar.gz tar -xzf ${TEMP_DIR}/k9s_Linux_amd64.tar.gz --directory ${TEMP_DIR} install -m 0755 -o root -g root ${TEMP_DIR}/k9s /usr/local/bin/k9s k9s completion bash \u0026gt; /etc/bash_completion.d/k9s minikube TEMP_DIR=\u0026#34;$(mktemp -d)\u0026#34; curl -Lo ${TEMP_DIR}/minikube_latest_amd64.deb https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb dpkg -i ${TEMP_DIR}/minikube_latest_amd64.deb minikube completion bash \u0026gt; /etc/bash_completion.d/minikube When Minikube starts it checks for a Docker network with the same name and creates it automatically if it doesn\u0026rsquo;t exist. By creating this network manually we can define a custom IP range and make the node IPs predictable.\ndocker network rm minikube \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 docker network create \u0026#34;minikube\u0026#34; --driver=\u0026#34;bridge\u0026#34; --subnet=\u0026#34;10.20.30.0/24\u0026#34; --gateway=\u0026#34;10.20.30.1\u0026#34; haproxy A simple reverse proxy to easily access HTTP workloads running on Minikube. It listens on ports 80 and 443 of the system and forwards connections to the Minikube network.\napt install -y haproxy systemctl daemon-reload systemctl enable haproxy.service systemctl stop haproxy.service echo \u0026#39;EXTRAOPTS=\u0026#34;-q\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/default/haproxy cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; /etc/haproxy/haproxy.cfg global log /dev/log local0 info defaults log global timeout client 30000 timeout connect 5000 timeout server 30000 frontend stats mode http bind *:8404 stats enable stats uri / stats refresh 30s no log frontend metrics mode http bind *:8405 http-request use-service prometheus-exporter no log frontend haproxy-http mode tcp bind *:80 default_backend minikube-http option tcplog frontend haproxy-https mode tcp bind *:443 default_backend minikube-https option tcplog backend minikube-http mode tcp server minikube 10.20.30.2:31080 check server minikube-m02 10.20.30.3:31080 check server minikube-m03 10.20.30.4:31080 check server minikube-m04 10.20.30.5:31080 check server minikube-m05 10.20.30.6:31080 check backend minikube-https mode tcp server minikube 10.20.30.2:31443 check server minikube-m02 10.20.30.3:31443 check server minikube-m03 10.20.30.4:31443 check server minikube-m04 10.20.30.5:31443 check server minikube-m05 10.20.30.6:31443 check EOF haproxy -c -f /etc/haproxy/haproxy.cfg systemctl start haproxy.service helm curl -L https://packages.buildkite.com/helm-linux/helm-debian/gpgkey | gpg --dearmor \u0026gt; /etc/apt/keyrings/helm.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/helm.gpg] https://packages.buildkite.com/helm-linux/helm-debian/any any main\u0026#34; \u0026gt; /etc/apt/sources.list.d/helm.list apt update \u0026amp;\u0026amp; apt install -y helm helm completion bash \u0026gt; /etc/bash_completion.d/helm terraform USERNAME=\u0026#34;user\u0026#34; curl -L https://apt.releases.hashicorp.com/gpg | gpg --dearmor \u0026gt; /etc/apt/keyrings/hashicorp.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/hashicorp.gpg] https://apt.releases.hashicorp.com $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${VERSION_CODENAME}\u0026#34;) main\u0026#34; \u0026gt; /etc/apt/sources.list.d/hashicorp.list apt update \u0026amp;\u0026amp; apt install -y terraform echo \u0026#39;complete -C /usr/bin/terraform terraform\u0026#39; \u0026gt;\u0026gt; /home/${USERNAME}/.bashrc grafana curl -L https://apt.grafana.com/gpg.key | gpg --dearmor \u0026gt; /etc/apt/keyrings/grafana.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main\u0026#34; \u0026gt; /etc/apt/sources.list.d/grafana.list apt update \u0026amp;\u0026amp; apt install -y grafana systemctl daemon-reload systemctl enable grafana-server.service systemctl stop grafana-server.service cp /etc/grafana/grafana.ini /etc/grafana/grafana.ini.bak chown :grafana /etc/grafana/grafana.ini.bak sed -i \u0026#39;s/^;reporting_enabled = true$/reporting_enabled = false/\u0026#39; /etc/grafana/grafana.ini systemctl start grafana-server.service The web interface is on port 3000, login with admin as the username and password.\nact TEMP_DIR=\u0026#34;$(mktemp -d)\u0026#34; ACT_VERSION=\u0026#34;$(curl -Ls https://api.github.com/repos/nektos/act/releases/latest | jq -r .tag_name)\u0026#34; curl -Lo ${TEMP_DIR}/act_Linux_x86_64.tar.gz https://github.com/nektos/act/releases/download/${ACT_VERSION}/act_Linux_x86_64.tar.gz tar -xzf ${TEMP_DIR}/act_Linux_x86_64.tar.gz --directory ${TEMP_DIR} install -m 0755 -o root -g root ${TEMP_DIR}/act /usr/local/bin/act Tactical Enhancements This part of the guide covers things that are nice to have but not strictly necessary.\nA nice upgrade is to register a domain. With a domain we can set up DNS records that take advantage of the global DNS network and also issue valid TLS certificates.\nAfter registering a domain through a registrar, we create a Cloudflare account and transfer the domain\u0026rsquo;s DNS management to it. On Cloudflare, we configure A records that point to the local IP of our system and set up an API token with \u0026ldquo;Edit zone DNS\u0026rdquo; permissions..\ndomain.xyz A 192.168.1.3 *.domain.xyz A 192.168.1.3 With the API token we run a script that uses acme.sh to generate a wildcard TLS certificate and save it to the ~/.tls directory of our system.\nTo download the script run the following..\nUSERNAME=\u0026#34;user\u0026#34; curl -Lo /home/${USERNAME}/.scripts/acme.sh https://blog.korliaftis.com/scripts/acme.sh chmod +x /home/${USERNAME}/.scripts/acme.sh Before running the script, make sure to edit it and update the following variables..\nUSERNAME=\u0026#34;\u0026#34; # your debian username - \u0026#34;user\u0026#34; DOMAIN_NAME=\u0026#34;\u0026#34; # your domain name - \u0026#34;domain.xyz\u0026#34; API_TOKEN=\u0026#34;\u0026#34; # your cloudflare api token - \u0026#34;6_z3K2aFdSpIq6VBkaz3i4xHT_uYiBNmDftWNZ-K\u0026#34; Another useful upgrade to consider is setting up remote access. We could rent a VPS and set up WireGuard tunnels between our devices but this adds latency. A better option is to use something like Nebula where the VPS can act as a broker and not as a relay. Another valid option would be to use Cloudflare tunnels. However, the simplest option is to use a managed service like Tailscale or ZeroTier which can handle everything for us.\nThe last upgrade to consider is configuring wake-on-lan. Most systems support this feature but it must be enabled in both the BIOS and on the network interface. We use ethtool to check if wake-on-lan is enabled on our system\u0026rsquo;s ethernet interface..\nINTERFACE=\u0026#34;\u0026#34; # the name of the ethernet interface - enp2s0 ethtool ${INTERFACE} | grep \u0026#34;Wake-on:\u0026#34; If the output is Wake-on: g it means that wake-on-lan is already enabled, if not we do the following to enable it..\nINTERFACE=\u0026#34;\u0026#34; # the name of the ethernet interface - enp2s0 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/network/interfaces.d/${INTERFACE} auto ${INTERFACE} iface ${INTERFACE} inet dhcp ethernet-wol g EOF systemctl restart networking.service ethtool ${INTERFACE} | grep \u0026#34;Wake-on:\u0026#34; If we pair wake-on-lan with a Raspberry Pi on the same network and a remote access solution we will be able to power on our system and control it from anywhere.\nOngoing Maintenance Setting things up once is fine but without updates we are stuck with a snapshot of the system from the day it was installed.\nRunning apt update \u0026amp;\u0026amp; apt upgrade -y regularly will keep both the system and most of our tools up to date. To update kubectl, k9s, minikube and act we can simply run their installation commands again.\nThe TLS certificate is valid for 90 days and we can issue a fresh certificate by running the script again.\nClosing Thoughts Allright, we saw a bunch of things and touched many topics but one thing we did not cover is security.\nSo is it secure? Well, we prioritized convenience over security in a few areas but at the end of the day this is a homelab not a production system, it will never store sensitive data and if someone breaks into my house to steal my stuff my crusty old hardware will be the least of my concerns.\nAt the end of the day this is all about experimentation and learning so take what works for you and make it your own.\n","permalink":"https://korliaftis.github.io/posts/laying-the-foundation.html","summary":"\u003cp\u003eSince I started working with computers I always loved the idea of a homelab, a safe space to mess with stuff, break stuff and sometimes learn a few things along the way.\u003c/p\u003e\n\u003cp\u003eBack when I was trying to land my first tech job my homelab was a bunch of noisy Cisco switches and routers. Along the way I discovered GNS3 and eventually those physical switches and routers were virtualized on my Sony Vaio VGN-N31Z with it\u0026rsquo;s Core 2 Duo, 2GB of RAM and 120GB of spinning rust.\u003c/p\u003e","title":"Laying the Foundation"}]